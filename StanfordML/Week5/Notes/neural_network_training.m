%%% Training Neural Networks

%% Neural Network Cost Function
% For binary classification, there is one output which maps to 1 if the...
% ...positive class is predicted, 0 if negative
% For multiclass classification, there is an output node for each class 
% See the 3:00 of the "cost function" video for the neural network cost function
% Neural network cost functions can include a regularization parameter lambda

%% Backpropagation Algorithm
% Forward propagation is the process of taking the theta & x matrices and...
% ...executing operations to make a prediction
% In order to calculate derivatives, backpropagation is used
% The error of node i at layer j, delta, is equal to what the neural network...
% ...predicts for that training sample value minus the actual value. This...
% ...error term is then used to recursively calculate the first delta value...
% ...as presented at 6:00 of the "backpropagation algorithm" video 
% Delta is the partial derivative of the cost function
% Backpropagation refers to the fact that we calculate delta for the final...
% ...layer first, then we recursively calculate up to the first layer.
% The derivation of backpropagation for neural network is very involved
% The pseudocode for the backpropagation algorithm appears at 10:00 of the...
% ..."backpropagation algorithm" video

%% Backpropagation in Practice
% Unrolling is the process of collapsing multiple matrices into 1 column vector
% Unrolling makes passing the theta argument into functions easier; the...
% ...original thetas can be reconstructed by using the Octave "reshape"...
% ...command and knowning the indices in the vector of each theta matrix 
% Gradient checking makes sure that backpropagation algorithms are working...
% ...correctly, since it's easy to have bugs in such a complex algorithm 
% Gradient checking does a finite difference approx of the derivative and...
% ...check to make sure that this approximately equals the delta calculated...
% ...derivative from backpropagation
% Once the algorithm is tested using gradient checking, turn it off. This...
% ...makes the algorithm too computationally expensive
% Zero initialization of thetas for neural networks do not work; this results...
% ...in each outflow from the input layer equalling so the hidden layers...
% ...will equal; this means that the partial derivatives will be constrained...
% ...to being the same value, so all units calculate using the same feature
% This means that input thetas cannot be equal to result in interesting...
% ...functions; break the symmetry by setting thetas equal to random uniforms

%% Training a Neural Network
% Pick the neural network architecture (the amount of layers and units at...
% ...each layer)
% The # of inputs in a neural network is the amount of features 
% The number of output units is the number of classes 
% A reasonable default is to use only 1 hidden layer but if you use more...
% ...have the same amount of units in each hidden layer 
% The number of hidden units in the hidden layer is typically equal to the...
% ...number of features in the input layer  
% 1. Randomly initialize weights
% 2. Implement forward prop to get the h(x(i)) for all x(i)
% 3. Implement code to calculate the cost function 
% 4. Implement back prop to calculate each delta 
% 5. Use gradient checking to compare delta to numerical estimates 
% 6. Use gradient descent to minimize the cost function as a function of theta 
% 7. The neural network cost function is non-convex, so converging to local...
% ...optima is possible but not likely in most situations 


