%%% Classification Introduction
% Classification is an ML problem where the outcome can take only a...
% ...countable set of outcomes
% Examples: Is this email spam or not? Is this transaction fraudulent or not?
% Typically the "negative class" (e.g. not spam) is 0 while the "positive...
% ...class" is 1, which indicates the presence of something
% Linear regression could be fitted with anything greater than 0.5 fit being...
% ...classified as a "positive class", otherwise "negative class"
% Linear regression typically fails for classification because outliers...
% ...that don't add new information can greatly skew outcomes
% Linear regression can also fit values as less than 0 or greater than 1
% Logistic regression guarentees that outputs are bounded between 0 and 1

%%% Classification Hypothesis Representation
% The Sigmoid/Logistic function takes 1 and divides by 1 plus the...
% ...exponential of the negative of the fitted values; this means that as...
% ...fitted values approach negative infinity their classification converges...
% ...to 0 and as they approach infinity the logistic function converges to 1
% The outputted value is the probability the outcome is 1 given the features...
% ...of that observation

%%% Decision Boundary
% Classification involves mapping all possible range of continuous outcomes...
% ...each to one category of outcomes based on a decision boundary/rule
% In linear regression, we predict positive classes when prediction is...
% ...greater than 0.5 & negative class, otherwise
% Transforming to the logistic function, positive classes are predicted when...
% ...the logistic function is greater than 0 & negative class, otherwise
% Decision boundaries can be non-linear when polynomial terms are fitted
% Decision boundaries are a property of the hypothesis and not the training set

