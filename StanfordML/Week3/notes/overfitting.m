%%% Overfitting

%% Overfitting Intro
% A model has high bias/underfitting if there are many terms needed to...
% ...still accurately fit the model, so much signal is left unmodelled
% A model has high variance/overfitting if the variable has too many features...
% ...so it just fits the training set well, and thus models training data noise
% The bias variance tradeoff means that the training set should be fitted...
% ...such that it captures signal innate to the population, but not include...
% ...so many features such that it captures the noise of that particular sample

%% Addressing overfitting
% 1. Reduce the # of features via manual inspection or model selection algorithm
% 2. Regularization - keep all features but reduce magnitude of parameters...
% ...that are estimated
% Small values for estimated parameters leads to a "simpler" hypothesis that...
% ...is less prone to overfitting 
% Regularization convention does not shrink the constant, just slope parameters
% Add a regularization parameter called lambda that adds a function of...
% ...the parameters squared into the cost function; this must be minimized so...
% ...estimated theta parameters will be smaller than they would be otherwise
% If lambda is arbitrarily large, then underfitting will occur because...
% ...minimized thetas for the fit that minimizes cost function will be zeros

%%% Regularized Linear Regression
% Alter the traditional gradient descent algorithm (week 1) by adding inner...
% ...slope terms by theta parameter multiplied by lambda / m
% The term above is the partial derivative of the cost function with...
% ...regularized regression
% Gradient descent shrinks the theta parameter at the end, see 5:00 of the...
% ..."regularized linear regression" video
% See 7:00 for the way to execute regularized linear regression for the...
% ...normal equation
% If lambda is greater than zero, it can be proven that the inverted matrix...
% ...in the normal equation is invertible

%%% Regularized Logistic Regression
% As in linear regression, the update step of the gradient descent algorithm...
% ...for a slope parameter adds (lambda / m) times the current theta value to...
% ...the existing cost function as mentioned in week 1 
% This update is the partial derivative of adding the sum of squares of the \
% ...parameter times (lambda / 2m) w.r.t. theta; recall this is the added...
% ...punishment to the cost function for large parameter values
