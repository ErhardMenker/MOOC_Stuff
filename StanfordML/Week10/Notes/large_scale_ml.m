%%% Large Scale Machine Learning

%% Motivation
% ML algorithms work better today because data sets are much larger
% Low bias learning algorithms on large data sets often work very well
% "It's not who has the best algorithm, but who has the most data"
% ML algorithms on big data can be very computationally expensive if inefficient
% High variance learning algorithms are well suited for big data; this...
% ...reduces the probability of overfitting because more data is seen

%% Stochastic Gradient Descent
% Batch (normal) gradient descent does not scale easily for very large data...
% ...sets because for each feature, you must sum over all of its observations
% Stochastic gradient descent...
%   1) Randomly shuffles the observations to ensure no sorting occurred
%   2) Update the feature parameters by iterating through each observation...
%      ...and only doing the update to the params based on what fits that obs 
% Stochastic gradient descent updates the parameters constantly as they are...
% ...fit to each observation; only after each obs is iterated thru are the...
% ...params updated for the 1st iteration in batch gradient descent
% The cost function will not necessarily decrease after each obs is iterated...
% ...thru in stochastic gradient descent
% After many observations, batch gradient descent should just update very...
% ...closely about params that minimize the learning algorithm's cost function
% Typically, each observation in batch gradient descent is iterated 1-10 times 
% Check if the stochastic gradient descent's cost function is decreasing to...
% ...ensure convergence every ~ 1000 examples iterated through 
% Often times, smaller learning rates will result in longer convergence but...
% ...the cost function will converge in the long run because anomalous...
% ...training obs do not "throw off" the parameters too strongly 
% If the cost function continues to increase, this is a sign that alpha is...
% ...too large -> reduce the learning rate 
% Unlike in batch, stochastic gradient descent can benefit from reducing...
% ...alpha over time as the parameters converge to the optimal solution to...
% ...prevent strange observations from shocking params significantly away...
% ...from the global minimum (setting a decrease rate for alpha can be tricky)

%% Mini-Batch Gradient Descent 
% Batch gradient descent uses all m observations in each param iteration...
% ...while stochastic only uses 1, mini-batch uses b where 1 < b < m
% Typically, b is somewhere between 2 & 100
% Mini-batch gradient descent executes gradient descent by continuously...
% ...updating the theta parameters by fitting it over a new mini-batch of...
% ...observations of size b, ending a full round after each mini-batch is...
% ...iterated thru 
% Mini-batch gradient descent often just beats stochastic gradient descent if...
% ...vectorization is optimized (by looking at b obs instead of 1,...
% ...parallelization is possible and should be taken advantage of with highly...
% ...optimized built-in linear algebra libraries)

%% Online Learning
% Online learning is learning from data that continuously streams from users
% The assumption of online learning algorithms is that they "run forever"...
% ...because data are constantly streaming in
% Online learning algorithms (like gradient descent) will update to match the...
% ...preferences of the latest data providers
% Online learning data only needs to be processed once in history, then it is...
% ...baked into the params and not needed to be re-referenced
% Online learning is great for evolving pools of users because the learned...
% ...hypothesis is slowly adapted with new users 

%% Map Reduce & Data Parallelism
% Map-reduce partitions the training set & executes the algorithm on each...
% ...partition in a unique machine, then combines the results at the end
% Without network latencies, you can get speedups times the # of computers...
% ...you split up the training set into 
% Map-reduce typically works when the algorithm can be expressed as taking...
% ...sums of functions over the training set 
% Map-reduce is possible on 1 computer if the computer has multiple cores...
% ...(work is sent to each core)
% Work done on multi-cores benefits from not having network latency costs
% Fundamentally, map-reduce allows the execution of algorithms on data...
% ...structures that would not be possible on only 1 machine 