%%% Support Vector Machines

%% Optimization of SVM
% For logistic regression, the cost function for both cases (when y actually...
% ...equals 0 or 1) increases exponentially as theta transpose * X projects...
% ...a value ever further into the incorrect prediction direction 
% The Support Vector Machine flats the line at cost equals 0 for a predicted...
% ...value when the predicted value is greater than 1 (y = 1) or less than -1...
% ...(y = 0), increasing the cost function as a straight line elsewhere
% This straight cost function allows for easier optimization
% The cost function of the negative (y = 0) & positive (y = 1) actual case...
% ...are called cost0 & cost1, respectively 
% SVMs do not have the (1 / m) term in the objective function from regression 
% SVMs attach the regularization term to the traditional cost portion of the...
% ...cost function, so this C term must be small for much regularization 

%% Large Margin Intuition 
% Recall that for SVMs, for y = 1 we want to predict >= 1 (not just 0) & for...
% ...y = 0, we want to predict <= - 1 (not just 0) to give safety margin factor
% The SVM is a large margin classifier because the algorithm gives a robust...
% ...partition between the positive & negative training outcomes that does...
% ...not necessarily occur with other ML algorithms like logistic regression
% The robustness from SVM comes from the larger margin given by what is...
% ...required to classify an outcome as + (z >= 1) & - (z <= -1), instead of...
% ...making the boundary about 0, as it is for logistic regression 
% Sufficient regularization helps in the presence of outliers & non-linearly...
% ...separable positive/negative classes 

%% Kernels Concept 
% In order to come up with sophisticated non-linear terms, higher order...
% ...polynomials (like quadratics) & interaction terms become necessary
% A kernel engineers new features for a predictor variable as a function of...
% ...2 predictors' similarity (common: Gaussian Kernel)
% The new features engineered for a predictor are the interaction of the...
% ...predictor with a "landmark" 
% Under the Gaussian Kernel, the kernel will be 1 if the the 2 features are...
% ...similar in values & 0 if the 2 features are far apart 
% Smaller values of sigma ^ 2 drop the kernel value more rapidly when moving...
% ...away from the point where the 2 features are equal for the Gaussian Kernel
% In practice, the landmarks are placed where the training examples are 
% Kernels can be used with non-SVM ML algorithms, but the computational...
% ...scaling that is in SVM is not generalized to other algorithms like...
% ...logistic regression 

%% Kernels Application
% Writing a custom made SVM cost function is not recommended 
% When using an SVM library, need to specify regularization C & choice of...
% ...kernel (linear/no kernel, Gaussian kernel)
% Note that linear kernel is the same as no kernel 
% Gaussian Kernels are preferable for many observations (m) but not many...
% ...features (n), so complex functions can be fit without fear of...
% ...overfitting because of many observations while allowing for a rich...
% ...non-linear explanation of how the few features impact the outcome variable
% Perform feature scaling before using the Gaussian kernel, otherwise the...
% ...norms will be dominated by the differences in the feature's units 
% Similarity functions fail if they do not satisfy Mercer's Theorem, which...
% ...causes them to diverge under most optimization algorithms 
% There are many esoteric kernels like the chi-squared & polynomial kernels
% Most SVM packages have multi-class classification capabilities, but use the...
% ...one-vs-all approach if the package does not 
% Use logistic regression or SVM with no kernel if there are many features...
% ...but not many observations, to avoid problem of non-linear overfitting 
% If there are not many features (1 - 1000) & intermediate # of obs...
% ...(10 - 10000), use SVM with a Gaussian kernel (need to really find true...
% ...nature of the relationship between the few features among the many obs)
% If there are not many features (1 - 1000) but many obs (10000 +), add more...
% manual features and use SVM without a kernal or logistic regression because...
% ...the Gaussian Kernel is computationally slow with many observations
% Logistic regression & SVM with no kernel are conceptually similar, the SVM...
% ...really benefits from the kernels because they allows for non-linear fit 
% Neural networks work in many of these cases but can be slow to train 
