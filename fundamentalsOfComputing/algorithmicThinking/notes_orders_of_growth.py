### Orders of Growth

# As an algorithm's input increases, how does the time of execution/number of required operations increase?
# Real life inputs are of a large magnitude, and this is what the algorithmic thinker cares about.
# Functions can grow in log, linear, quadratic, time, and so on with respect to their input size.
# It is not difficult to write algorithms that could take several years to calculate even with relatively small input size and high computing power.
# A program that runs in logarithmic time is great because it has a negative second derivative since it increases at a decreasing rate.
# When expressing growth of functions as a function of n, we care about the limit as n approaches infinity, which means lower order terms and constants lose importance.
# The point of knowing orders of growth is to get an approximation of how fast an algorithm takes as a function of inputs.
# This order of approximation allows people to conclude easily if an algorithm can execute in a reasonable amount of time.
# Running time of an algorithm can be plotted as a function of the algorithm's input size.

